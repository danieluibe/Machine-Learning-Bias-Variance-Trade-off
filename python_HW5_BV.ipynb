{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<body>\n",
    "<h2>Project 5: Bias Variance Trade-off</h2>\n",
    "\n",
    "<!--announcements-->\n",
    "<blockquote>\n",
    "    <center>\n",
    "    <a href=\"http://blogs.worldbank.org/publicsphere/files/publicsphere/biased_processing.jpg\"><img src=\"bias.jpg\" width=\"600px\" /></a>\n",
    "    </center>\n",
    "      <p><cite><center>\"All of us show bias when it comes to what information we take in.<br>We typically focus on anything that agrees with the outcome we want.\"<br>\n",
    "<b>--Noreena Hertz</b>\n",
    "      </center></cite></p>\n",
    "</blockquote>\n",
    "<h3>Introduction</h3>\n",
    "\n",
    "<p>\n",
    "Recall that the squared error can be decomposed into <em>bias</em>, <em>variance</em> and <em>noise</em>: \n",
    "$$\n",
    "    \\underbrace{\\mathbb{E}[(h_D(x) - y)^2]}_\\mathrm{Error} = \\underbrace{\\mathbb{E}[(h_D(x)-\\bar{h}(x))^2]}_\\mathrm{Variance} + \\underbrace{\\mathbb{E}[(\\bar{h}(x)-\\bar{y}(x))^2]}_\\mathrm{Bias} + \\underbrace{\\mathbb{E}[(\\bar{y}(x)-y(x))^2]}_\\mathrm{Noise}\\nonumber\n",
    "$$\n",
    "We will now create a data set for which we can approximately compute this decomposition. \n",
    "The function <em><strong>toydata</strong></em> generates a binary data set with class $1$ and $2$. Both are sampled from Gaussian distributions:\n",
    "$$\n",
    "p(\\vec x|y=1)\\sim {\\mathcal{N}}(0,{I}) \\textrm { and } p(\\vec x|y=2)\\sim {\\mathcal{N}}(\\mu_2,{I}),\n",
    "$$\n",
    "\n",
    "where $\\mu_2=[2;2]^\\top$ (the global variable <em>OFFSET</em> $\\!=\\!2$ regulates these values: $\\mu_2=[$<em>OFFSET</em> $;$ <em>OFFSET</em>$]^\\top$).\n",
    "</p>\n",
    "\n",
    "<h3>Computing noise, bias and variance</h3>\n",
    "<p>\n",
    "You will need to edit four functions:  <em><strong>computeybar</strong></em>,  <em><strong>computehbar</strong></em>,  <em><strong>computevariance</strong></em> and  <em><strong>biasvariancedemo</strong></em>. First take a look at <strong>biasvariancedemo</strong> and make sure you understand where each function should be called and how they contribute to the Bias/Variance/Noise decomposition. <br/><br/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Libraries**: Before we get started we need to install a few libraries. You can do this by executing the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>\n",
    "import numpy as np\n",
    "from numpy.matlib import repmat\n",
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('PDF')\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from scipy.io import loadmat\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L2Distance Helper Function**: L2Distance is a helper function used in our implementation of the ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l2distance(X, Z=None):\n",
    "    \"\"\"\n",
    "    function D=l2distance(X,Z)\n",
    "\n",
    "    Computes the Euclidean distance matrix.\n",
    "    Syntax:\n",
    "    D=l2distance(X,Z)\n",
    "    Input:\n",
    "    X: dxn data matrix with n vectors (columns) of dimensionality d\n",
    "    Z: dxm data matrix with m vectors (columns) of dimensionality d\n",
    "\n",
    "    Output:\n",
    "    Matrix D of size nxm\n",
    "    D(i,j) is the Euclidean distance of X(:,i) and Z(:,j)\n",
    "\n",
    "    call with only one input:\n",
    "    l2distance(X)=l2distance(X,X)\n",
    "    \"\"\"\n",
    "    if Z is None:\n",
    "        n, d = X.shape\n",
    "        s1 = np.sum(np.power(X, 2), axis=1).reshape(-1,1)\n",
    "        D1 = -2 * np.dot(X, X.T) + repmat(s1, 1, n)\n",
    "        D = D1 + repmat(s1.T, n, 1)\n",
    "        np.fill_diagonal(D, 0)\n",
    "        D = np.sqrt(np.maximum(D, 0))\n",
    "    else:\n",
    "        n, d = X.shape\n",
    "        m, _ = Z.shape\n",
    "        s1 = np.sum(np.power(X, 2), axis=1).reshape(-1,1)\n",
    "        s2 = np.sum(np.power(Z, 2), axis=1).reshape(1,-1)\n",
    "        D1 = -2 * np.dot(X, Z.T) + repmat(s1, 1, m)\n",
    "        D = D1 + repmat(s2, n, 1)\n",
    "        D = np.sqrt(np.maximum(D, 0))\n",
    "    return D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Toydata Helper Function**: Toydata is a helper function used to generate the the binary data with n/2 values in class 1 and n/2 values in class 2. With class 1 being the label for data drawn from a normal distribution having mean 0 and sigma 1. And clss 2 being the label for data drawn from a normal distribution with mean OFFSET and sigma 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def toydata(OFFSET,N):\n",
    "    \"\"\"\n",
    "    function [x,y]=toydata(OFFSET,N)\n",
    "    \n",
    "    This function constructs a binary data set. \n",
    "    Each class is distributed by a standard Gaussian distribution.\n",
    "    INPUT: \n",
    "    OFFSET:  Class 1 has mean 0,  Class 2 has mean 0+OFFSET (in each dimension). \n",
    "    N: The function returns N data points ceil(N/2) are of class 2, the rest\n",
    "    of class 1\n",
    "    \"\"\"\n",
    "    \n",
    "    NHALF = int(np.ceil(N/2))\n",
    "    x = np.random.randn(N, 2)\n",
    "    x[NHALF:, :] += OFFSET  \n",
    "    \n",
    "    y = np.ones(N)\n",
    "    y[NHALF:] *= 2\n",
    "    \n",
    "    jj = np.random.permutation(N)\n",
    "    return x[jj, :], y[jj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "(a) <strong>Noise:</strong> First we focus on the noise. For this, you need to compute $\\bar y(\\vec x)$ in  <em><strong>computeybar</strong></em>. You can compute the probability $p(\\vec x|y)$ with the equations $p(\\vec x|y=1)\\sim {\\mathcal{N}}(0,{I}) \\textrm { and } p(\\vec x|y=2)\\sim {\\mathcal{N}}(\\mu_2,{I})$. Then use Bayes rule to compute $p(y|\\vec x)$. <br/><br/>\n",
    "<strong>Note:</strong> You may want to use the function <em>normpdf</em>, which is defined for  you in <em><strong>computeybar</strong></em>.\n",
    "<br/><br/></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computeybar(xTe, OFFSET):\n",
    "    \"\"\"\n",
    "    function [ybar]=computeybar(xTe, OFFSET);\n",
    "\n",
    "    computes the expected label 'ybar' for a set of inputs x\n",
    "    generated from two standard Normal distributions (one offset by OFFSET in\n",
    "    both dimensions.)\n",
    "\n",
    "    INPUT:\n",
    "    xTe : nx2 array of n vectors with 2 dimensions\n",
    "    OFFSET    : The OFFSET passed into the toyData function. The difference in the\n",
    "                mu of labels class1 and class2 for toyData.\n",
    "\n",
    "    OUTPUT:\n",
    "    ybar : a nx1 vector of the expected labels for vectors xTe\n",
    "    \"\"\"\n",
    "    n,temp = xTe.shape\n",
    "    ybar = np.zeros(n)\n",
    "    \n",
    "    # Feel free to use the following function to compute p(x|y), or not\n",
    "    # normal distribution is default mu = 0, sigma = 1.\n",
    "    normpdf = lambda x, mu, sigma: np.exp(-0.5 * np.power((x - mu) / sigma, 2)) / (np.sqrt(2 * np.pi) * sigma)\n",
    "    \n",
    "    ## fill in code here\n",
    "    raise NotImplementedError('Your code goes here!')\n",
    "    \n",
    "    return ybar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing the Data**:\n",
    "You can now see the error of the bayes classifier. Below is a plotting of the two classes of points and the misclassified points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "OFFSET = 2\n",
    "xTe, yTe = toydata(OFFSET, 1000)\n",
    "\n",
    "# compute Bayes Error\n",
    "ybar = computeybar(xTe, OFFSET)\n",
    "predictions = np.round(ybar)\n",
    "errors = predictions != yTe\n",
    "err = errors.sum() / len(yTe) * 100\n",
    "print('Error of Bayes classifier: %.1f%%.' % err)\n",
    "\n",
    "# plot data\n",
    "i1 = yTe == 1\n",
    "i2 = yTe == 2\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(xTe[i1, 0], xTe[i1, 1], c='r', marker='o')\n",
    "plt.scatter(xTe[i2, 0], xTe[i2, 1], c='b', marker='o')\n",
    "plt.scatter(xTe[errors, 0], xTe[errors, 1], c='k', s=100, alpha=0.2)\n",
    "plt.title(\"Plot of data (misclassified points highlighted)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>With the help of <strong>computeybar</strong> you can now compute the \"noise\" variable within <strong>biasvariancedemo</strong>. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**kregression Helper Function**: \n",
    "<br/>\n",
    "<strong>Important</strong> - $h_D$ is defined for you in <em><strong>kregression</strong></em>. It's kernelized ridge regression with kernel width $\\sigma$ and regularization constant $\\lambda$.\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kregression(xTr,yTr,sigma=0.1,lmbda=0.01):\n",
    "    \"\"\"\n",
    "    function kregression(xTr,yTr,sigma,lmbda)\n",
    "    \n",
    "    Input:\n",
    "    xTr | training data (nx2)\n",
    "    yTr | training labels (nx1)\n",
    "    sigma | kernel width (>0)\n",
    "    lmbda | regularization constant (>0)\n",
    "    \n",
    "    Output:\n",
    "    fun | usage: predictions=fun(xTe);\n",
    "    \"\"\"\n",
    "    kernel = lambda x, z: np.power(1+(np.power(l2distance(x,z),2) / (2 * np.power(sigma,2))),-4)\n",
    "    ridge = lambda K, lmbda2: K + lmbda * np.eye(K.shape[0], K.shape[1])\n",
    "    beta = np.linalg.solve(ridge(kernel(xTr, xTr), lmbda), yTr)\n",
    "    \n",
    "    fun = lambda Xt: np.dot(kernel(Xt, xTr), beta)\n",
    "    return fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "(b) <strong>Bias:</strong> For the bias, you will need $\\bar{h}$. Although we cannot compute the expected value  $\\bar h\\!=\\!\\mathbb{E}[h]$, we can approximate it by training many $h_D$ and averaging their predictions. Edit the file <em><strong>computehbar</strong></em>. Average over <em>NMODELS</em> different $h_D$, each trained on a different data set of <em>Nsmall</em> inputs drawn from the same distribution. Feel free to call <em><strong>toydata</strong></em> to obtain more data sets. <br/><br/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def computehbar(xTe, sigma, lmbda, Nsmall, NMODELS, OFFSET):\n",
    "    \"\"\"\n",
    "    function [hbar]=computehbar(xTe, sigma, lmbda, NSmall, NMODELS, OFFSET);\n",
    "\n",
    "    computes the expected prediction of the average classifier (hbar)\n",
    "    for data set xTe. \n",
    "\n",
    "    The training data of size Nsmall is drawn from toydata with OFFSET \n",
    "    with kernel regression with sigma and lmbda\n",
    "\n",
    "    The \"infinite\" number of models is estimated as an average over NMODELS. \n",
    "\n",
    "    INPUT:\n",
    "    xTe       | nx2 matrix, of n column-wise input vectors (each 2-dimensional)\n",
    "    sigma     | kernel width of the RBF kernel\n",
    "    lmbda     | regularization constant\n",
    "    NModel    | Number of Models to average over\n",
    "    OFFSET    | The OFFSET passed into the toyData function. The difference in the\n",
    "                mu of labels class1 and class2 for toyData.\n",
    "    OUTPUT:\n",
    "    hbar | nx1 vector with the predictions of hbar for each test input\n",
    "    \"\"\"\n",
    "    n = xTe.shape[0]\n",
    "    hbar = np.zeros(n)\n",
    "    for j in range(NMODELS):\n",
    "        ## fill in code here\n",
    "        raise NotImplementedError('Your code goes here!')\n",
    "    hbar /= NMODELS\n",
    "    return hbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>With the help of <strong>computehbar</strong> you can now compute the \"bias\" variable within <strong>biasvariancedemo</strong>. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>(c) <strong>Variance:</strong> Finally, to compute the variance, we need to compute the term $\\mathbb{E}[(h_D-\\bar{h})^2]$. Once again, we can approximate this term by averaging over  <em>NMODELS</em> models. Edit the file <em><strong>computevariance</strong></em>. \n",
    "<br/></br></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computevariance(xTe, sigma, lmbda, hbar, Nsmall, NMODELS, OFFSET):\n",
    "    \"\"\"\n",
    "    function variance=computevariance(xTe,sigma,lmbda,hbar,Nsmall,NMODELS,OFFSET)\n",
    "\n",
    "    computes the variance of classifiers trained on data sets from\n",
    "    toydata.m with pre-specified \"OFFSET\" and \n",
    "    with kernel regression with sigma and lmbda\n",
    "    evaluated on xTe. \n",
    "    the prediction of the average classifier is assumed to be stored in \"hbar\".\n",
    "\n",
    "    The \"infinite\" number of models is estimated as an average over NMODELS. \n",
    "\n",
    "    INPUT:\n",
    "    xTe       : nx2 matrix, of n column-wise input vectors (each 2-dimensional)\n",
    "    sigma     : kernel width of the RBF kernel\n",
    "    lmbda     : regularization constant\n",
    "    hbar      : nx1 vector of the predictions of hbar on the inputs xTe\n",
    "    Nsmall    : Number of samples drawn from toyData for one model\n",
    "    NModel    : Number of Models to average over\n",
    "    OFFSET    : The OFFSET passed into the toyData function. The difference in the\n",
    "                mu of labels class1 and class2 for toyData.\n",
    "    \"\"\"\n",
    "    n = xTe.shape[0]\n",
    "    variance = np.zeros(n)\n",
    "    \n",
    "    for j in range(NMODELS):\n",
    "        ## fill in code here\n",
    "        raise NotImplementedError('Your code goes here!')\n",
    "    \n",
    "    variance = np.mean(variance)/NMODELS\n",
    "    return variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>With the help of <strong>computevariance</strong> you can now compute the \"variance\" variable within <strong>biasvariancedemo</strong>. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>If you did everything correctly and call execute the following demo. You should see how the error decomposes (roughly) into bias, variance and noise when regularization constant $\\lambda$ increases.</p>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# biasvariancedemo\n",
    "\n",
    "# how big is the training set size N\n",
    "Nsmall = 10\n",
    "# how big is a really big data set (approx. infinity)\n",
    "Nbig = 10000\n",
    "# how many models do you want to average over\n",
    "NMODELS = 100\n",
    "# What regularization constants to evaluate\n",
    "lmbdas = np.arange(-6, 0+0.5, 0.5)\n",
    "# what is the kernel width?\n",
    "sigma = 4\n",
    "\n",
    "# we store\n",
    "Nlambdas = len(lmbdas)\n",
    "lbias = np.zeros(Nlambdas)\n",
    "lvariance = np.zeros(Nlambdas)\n",
    "ltotal = np.zeros(Nlambdas)\n",
    "lnoise = np.zeros(Nlambdas)\n",
    "lsum = np.zeros(Nlambdas)\n",
    "\n",
    "# Different regularization constant classifiers\n",
    "for md in range(Nlambdas):\n",
    "    lmbda = 2 ** lmbdas[md]\n",
    "    # use this data set as an approximation of the true test set\n",
    "    xTe,yTe = toydata(OFFSET,Nbig)\n",
    "    \n",
    "    # Estimate AVERAGE ERROR (TOTAL)\n",
    "    total = 0\n",
    "    for j in range(NMODELS):\n",
    "        xTr2,yTr2 = toydata(OFFSET,Nsmall)\n",
    "        fsmall = kregression(xTr2,yTr2,sigma,lmbda)\n",
    "        total += np.mean((fsmall(xTe) - yTe) ** 2)\n",
    "    total /= NMODELS\n",
    "    \n",
    "    # Estimate Noise\n",
    "    ybar = computeybar(xTe, OFFSET)\n",
    "    noise = np.mean((yTe-ybar) ** 2)\n",
    "    \n",
    "    # Estimate Bias\n",
    "    hbar = computehbar(xTe,sigma, lmbda, Nsmall, NMODELS, OFFSET)\n",
    "    bias = np.mean((hbar-ybar) ** 2)\n",
    "    \n",
    "    # Estimating VARIANCE\n",
    "    variance = computevariance(xTe,sigma,lmbda,hbar, Nsmall, NMODELS, OFFSET)\n",
    "    \n",
    "    # print and store results\n",
    "    lbias[md] = bias\n",
    "    lvariance[md] = variance\n",
    "    ltotal[md] = total\n",
    "    lnoise[md] = noise\n",
    "    lsum[md] = lbias[md]+lvariance[md]+lnoise[md]\n",
    "    print('Regularization Î»=2^%2.1f: Bias: %2.4f Variance: %2.4f Noise: %2.4f Bias+Variance+Noise: %2.4f Test error: %2.4f'\n",
    "          % (lmbdas[md],lbias[md],lvariance[md],lnoise[md],lsum[md],ltotal[md]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot results\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(lbias[:Nlambdas],c='r',linestyle='-',linewidth=2)\n",
    "plt.plot(lvariance[:Nlambdas],c='k', linestyle='-',linewidth=2)\n",
    "plt.plot(lnoise[:Nlambdas],c='g', linestyle='-',linewidth=2)\n",
    "plt.plot(ltotal[:Nlambdas],c='b', linestyle='-',linewidth=2)\n",
    "plt.plot(lsum[:Nlambdas],c='k', linestyle='--',linewidth=2)\n",
    "\n",
    "plt.legend([\"Bias\",\"Variance\",\"Noise\",\"Test error\",\"Bias+Var+Noise\"]);\n",
    "plt.xlabel(\"Regularization $\\lambda=2^x$\",fontsize=18);\n",
    "plt.ylabel(\"Squared Error\",fontsize=18);\n",
    "plt.xticks([i for i in range(Nlambdas)],lmbdas);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to modify $\\lambda$/$\\sigma$ in these two files. If you want the approximation to be more accurate, increase <em>NMODELS</em> and/or <em>Nbig</em> (the more models you train, the better your approximation will be for $\\mathbb{E}[h]$ and $\\mathbb{E}[(h_D-\\bar{h})^2]$). \n",
    "You can also play around with the variable <em>Nsmall</em> which regulates how big your actual training is supposed to be. \n",
    "</p>\n",
    "\n",
    "\n",
    "<h3>Note</h3>\n",
    "<p>\n",
    "When computing the bias and variance, you approximate the results by training many $h_D$. We set <em>NMODELS</em>=1000 and use some thresholds to test if your functions' results are correct. Unfortunately, as a result of this randomness, there is still a small chance that you will fail some test cases, even though your implementations are correct. <br/><br/>\n",
    "If you can pass all the tests most of the times locally, then you are fine. In this case, if the autograder says your accuracy is not 100%, just commit the code again.<br/><br/>\n",
    "\n",
    "There is no competition this time.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
