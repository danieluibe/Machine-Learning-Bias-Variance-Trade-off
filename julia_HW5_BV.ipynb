{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "4c7fe1c5-2c94-4180-9c68-369aa869fc3c"
    }
   },
   "source": [
    "<body>\n",
    "<h2>Project 5: Bias Variance Trade-off</h2>\n",
    "\n",
    "<!--announcements-->\n",
    "<blockquote>\n",
    "    <center>\n",
    "    <a href=\"http://blogs.worldbank.org/publicsphere/files/publicsphere/biased_processing.jpg\"><img src=\"bias.jpg\" width=\"600px\" /></a>\n",
    "    </center>\n",
    "      <p><cite><center>\"All of us show bias when it comes to what information we take in.<br>We typically focus on anything that agrees with the outcome we want.\"<br>\n",
    "<b>--Noreena Hertz</b>\n",
    "      </center></cite></p>\n",
    "</blockquote>\n",
    "\n",
    "\n",
    "\n",
    "<h3>Introduction</h3>\n",
    "\n",
    "<p>\n",
    "Recall that the squared error can be decomposed into <em>bias</em>, <em>variance</em> and <em>noise</em>: \n",
    "$$\n",
    "    \\underbrace{\\mathbb{E}[(h_D(x) - y)^2]}_\\mathrm{Error} = \\underbrace{\\mathbb{E}[(h_D(x)-\\bar{h}(x))^2]}_\\mathrm{Variance} + \\underbrace{\\mathbb{E}[(\\bar{h}(x)-\\bar{y}(x))^2]}_\\mathrm{Bias} + \\underbrace{\\mathbb{E}[(\\bar{y}(x)-y(x))^2]}_\\mathrm{Noise}\\nonumber\n",
    "$$\n",
    "We will now create a data set for which we can approximately compute this decomposition. \n",
    "The function <em><strong>toydata</strong></em> generates a binary data set with class $1$ and $2$. Both are sampled from Gaussian distributions:\n",
    "$$\n",
    "p(\\vec x|y=1)\\sim {\\mathcal{N}}(0,{I}) \\textrm { and } p(\\vec x|y=2)\\sim {\\mathcal{N}}(\\mu_2,{I}),\n",
    "$$\n",
    "\n",
    "where $\\mu_2=[2;2]^\\top$ (the global variable <em>OFFSET</em> $\\!=\\!2$ regulates these values: $\\mu_2=[$<em>OFFSET</em> $;$ <em>OFFSET</em>$]^\\top$).\n",
    "</p>\n",
    "\n",
    "\n",
    "<h3>Computing noise, bias and variance</h3>\n",
    "<p>\n",
    "You will need to edit four functions:  <em><strong>computeybar</strong></em>,  <em><strong>computehbar</strong></em>,  <em><strong>computevariance</strong></em> and  <em><strong>biasvariancedemo</strong></em>. First take a look at <strong>biasvariancedemo</strong> and make sure you understand where each function should be called and how they contribute to the Bias/Variance/Noise decomposition. <br/><br/>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Libraries**: Before we get started we need to install a few libraries. You can do this by executing the following code. The first time you execute this code it may take a little while as it will install and compile the libraries on your computing node. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try\n",
    "    using PyPlot\n",
    "catch\n",
    "    Pkg.add(\"PyPlot\")\n",
    "    Pkg.build(\"PyPlot\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Toydata Helper Function**: Toydata is a helper function used to generate the the binary data with n/2 values in class 1 and n/2 values in class 2. With class 1 being the label for data drawn from a normal distribution having mean 0 and sigma 1. And clss 2 being the label for data drawn from a normal distribution with mean OFFSET and sigma 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "12b26090-fb01-4aa8-b6d4-0b4da00cf122"
    }
   },
   "outputs": [],
   "source": [
    "function toydata(OFFSET,N)\n",
    "    \"\"\" function (x,y)=toydata(OFFSET,N)\n",
    "        This function constructs a binary data set. \n",
    "        Each class is distributed by a standard Gaussian distribution.\n",
    "    \n",
    "    INPUT: \n",
    "        OFFSET:  Class 1 has mean 0,  Class 2 has mean 0+OFFSET (in each dimension). \n",
    "        N: The function returns N data boints ceil(N/2) are of class 2, the rest of class 1\n",
    "    \"\"\"\n",
    "    NHALF=Int(ceil(N/2));\n",
    "    x=randn(N,2);\n",
    "    x[NHALF:end,:]=x[NHALF:end,:]+OFFSET;\n",
    "    y=ones(N);\n",
    "    y[NHALF:end]=y[NHALF:end].*2;\n",
    "    jj=randperm(N);\n",
    "    x=x[jj,:];\n",
    "    y=y[jj];\n",
    "    return(x,y);\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "(a) <strong>Noise:</strong> First we focus on the noise. For this, you need to compute $\\bar y(\\vec x)$ in  <em><strong>computeybar</strong></em>. You can compute the probability $p(\\vec x|y)$ with the equations $p(\\vec x|y=1)\\sim {\\mathcal{N}}(0,{I}) \\textrm { and } p(\\vec x|y=2)\\sim {\\mathcal{N}}(\\mu_2,{I})$. Then use Bayes rule to compute $p(y|\\vec x)$. <br/><br/>\n",
    "<strong>Note:</strong> You may want to use the function <em>normpdf</em>, which is defined for  you in <em><strong>computeybar</strong></em>.\n",
    "<br/><br/></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "63d98758-e079-454b-b518-12f2069160d0"
    }
   },
   "outputs": [],
   "source": [
    "function computeybar(xTe)\n",
    "    \"\"\"\n",
    "    function [ybar]=computeybar(xTe);\n",
    "\n",
    "    computes the expected label 'ybar' for a set of inputs x\n",
    "    generated from two standard Normal distributions (one offset by OFFSET in\n",
    "    both dimensions.)\n",
    "\n",
    "    INPUT:\n",
    "    xTe    : nx2 array of n vectors with 2 dimensions\n",
    "    \n",
    "    Globals:\n",
    "    OFFSET : The OFFSET passed into the toyData function. The difference in the\n",
    "             mu of labels class1 and class2 for toyData.\n",
    "\n",
    "    OUTPUT:\n",
    "    ybar   : a nx1 vector of the expected labels for vectors xTe\n",
    "    \"\"\"\n",
    "    global OFFSET;\n",
    "    n,temp=size(xTe);\n",
    "    ybar=zeros(1,n);\n",
    "    \n",
    "    # Feel free to use the following function to compute p(x|y)\n",
    "    normpdf(x,mu,σ)=exp(-0.5*((x-mu)./σ).^2)./(sqrt(2*pi) .* σ);\n",
    "    \n",
    "    # FILL IN CODE HERE\n",
    "    throw(\"unimplemented\")\n",
    "    \n",
    "    return(ybar[:])\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing the Data**:\n",
    "You can now see the error of the bayes classifier. Below is a plotting of the two classes of points and the misclassified points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "d9ce0ef0-4427-46c6-81f8-6c295216d661"
    }
   },
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "global OFFSET;\n",
    "OFFSET=2;\n",
    "xTe,yTe=toydata(OFFSET,1000)\n",
    "\n",
    "# compute Bayes Error\n",
    "ybar=computeybar(xTe);\n",
    "predictions=round(ybar);\n",
    "errors=find(predictions.!=yTe);\n",
    "err=length(errors)/length(yTe)*100;\n",
    "println(\"Error of Bayes classifier: $err%.\");\n",
    "\n",
    "# plot data\n",
    "i1=(yTe.==1.0);\n",
    "i2=(yTe.==2.0);\n",
    "clf();\n",
    "plot(xTe[i1,1],xTe[i1,2],\"r.\");\n",
    "hold(true);\n",
    "plot(xTe[i2,1],xTe[i2,2],\"b.\");\n",
    "plot(xTe[errors,1],xTe[errors,2],\"ko\",markersize=10,alpha=0.2);\n",
    "title(\"Plot of data (misclassified points highlighted)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>With the help of <strong>computeybar</strong> you can now compute the \"noise\" variable within <strong>biasvariancedemo</strong>. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**kregression Helper Function**: \n",
    "<br/>\n",
    "<strong>Important</strong> - $h_D$ is defined for you in <em><strong>kregression</strong></em>. It's kernelized ridge regression with kernel width $\\sigma$ and regularization constant $\\lambda$.\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function kregression(xTr,yTr,σ=0.1,λ=0.01)\n",
    "    \"\"\"\n",
    "    function kregression(xTr,yTr,sigma,lmbda)\n",
    "    \n",
    "    Input:\n",
    "        xTr   | training data (nx2)\n",
    "        yTr   | training labels (nx1)\n",
    "        σ     | kernel width (>0)\n",
    "        λ     | regularization constant (>0)\n",
    "    \n",
    "    Output:\n",
    "        fun   | usage: predictions=fun(xTe);\n",
    "    \"\"\"\n",
    "    function sqdistance(X,Z)\n",
    "        n,d=size(X);\n",
    "        m=size(Z,1);\n",
    "        s1=sum(X.^2,2);\n",
    "        s2=sum(Z.^2,2);\n",
    "        D=broadcast(+,s1,broadcast(+,s2',-2.*X*Z'));\n",
    "        return(D);\n",
    "    end;\n",
    "    kernel(x,z)=(1+sqdistance(x,z)./(2*σ^2)).^(-4);\n",
    "    ridge(K,λ)=K+λ.*eye(size(K,1),size(K,2));\n",
    "    beta=ridge(kernel(xTr,xTr),λ)\\(yTr);\n",
    "    fun(Xt)=kernel(Xt,xTr)*beta;\n",
    "    return(fun);\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "b60c95e7-db87-4b31-8ec9-576a36767c69"
    }
   },
   "source": [
    "<p>\n",
    "(b) <strong>Bias:</strong> For the bias, you will need $\\bar{h}$. Although we cannot compute the expected value  $\\bar h\\!=\\!\\mathbb{E}[h]$, we can approximate it by training many $h_D$ and averaging their predictions. Edit the file <em><strong>computehbar</strong></em>. Average over <em>NMODELS</em> different $h_D$, each trained on a different data set of <em>Nsmall</em> inputs drawn from the same distribution. Feel free to call <em><strong>toydata</strong></em> to obtain more data sets. <br/><br/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "ce3f00aa-1d76-450a-bea5-1af18a72e073"
    }
   },
   "outputs": [],
   "source": [
    "function computehbar(xTe, sigma, lmbda)\n",
    "    \"\"\"\n",
    "    function [hbar]=computehbar(xTe, sigma, lmbda ; Nsmall, NModel, OFFSET);\n",
    "\n",
    "    computes the expected prediction of the average classifier (hbar)\n",
    "    for data set xTe. \n",
    "\n",
    "    The training data of size Nsmall is drawn from toydata with OFFSET \n",
    "    with kernel regression with sigma and lmbda\n",
    "\n",
    "    The \"infinite\" number of models is estimated as an average over NMODELS. \n",
    "\n",
    "    INPUT:\n",
    "        xTe    | nx2 matrix, of n column-wise input vectors (each 2-dimensional)\n",
    "        sigma  | kernel width of the RBF kernel\n",
    "        lmbda  | regularization constant\n",
    "    \n",
    "    Global:\n",
    "        NModel | Number of Models to average over\n",
    "        OFFSET | The OFFSET passed into the toyData function. The difference in the\n",
    "                 mu of labels class1 and class2 for toyData.\n",
    "    \n",
    "    OUTPUT:\n",
    "        hbar   | nx1 vector with the predictions of hbar for each test input\n",
    "    \"\"\"\n",
    "    global Nsmall,NMODELS,OFFSET\n",
    "    n=size(xTe,1);\n",
    "    hbar=zeros(n,1);\n",
    "    for j=1:NMODELS\n",
    "        # FILL IN CODES HERE\n",
    "        throw(\"unimplemented\")\n",
    "    end;\n",
    "    hbar=hbar./NMODELS;\n",
    "    return(hbar)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>With the help of <strong>computehbar</strong> you can now compute the \"bias\" variable within <strong>biasvariancedemo</strong>. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "89be1389-7c0d-4941-86e8-6608c1c44d6f"
    }
   },
   "source": [
    "\n",
    "<p>(c) <strong>Variance:</strong> Finally, to compute the variance, we need to compute the term $\\mathbb{E}[(h_D-\\bar{h})^2]$. Once again, we can approximate this term by averaging over  <em>NMODELS</em> models. Edit the file <em><strong>computevariance</strong></em>. \n",
    "<br/></br></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#<GRADED>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "2b6f1322-76ce-457d-aa89-3fb482891a1e"
    }
   },
   "outputs": [],
   "source": [
    "function computevariance(xTe, sigma, lmbda, hbar)\n",
    "    \"\"\"\n",
    "    function variance=computevariance(xTe,sigma,lmbda,hbar;Nsmall,NMODELS,OFFSET)\n",
    "\n",
    "    computes the variance of classifiers trained on data sets from\n",
    "    toydata.m with pre-specified \"OFFSET\" and \n",
    "    with kernel regression with sigma and lmbda\n",
    "    evaluated on xTe. \n",
    "    the prediction of the average classifier is assumed to be stored in \"hbar\".\n",
    "\n",
    "    The \"infinite\" number of models is estimated as an average over NMODELS. \n",
    "\n",
    "    INPUT:\n",
    "    xTe       | nx2 matrix, of n column-wise input vectors (each 2-dimensional)\n",
    "    sigma     | kernel width of the RBF kernel\n",
    "    lmbda     | regularization constant\n",
    "    hbar      | nx1 vector of the predictions of hbar on the inputs xTe\n",
    "    \n",
    "    Globals:\n",
    "    Nsmall    | Number of samples drawn from toyData for one model\n",
    "    NModel    | Number of Models to average over\n",
    "    OFFSET    | The OFFSET passed into the toyData function. The difference in the\n",
    "                mu of labels class1 and class2 for toyData.\n",
    "    \"\"\"\n",
    "    global Nsmall,NMODELS,OFFSET;\n",
    "    n=size(xTe,1);\n",
    "    variance=zeros(n,1);\n",
    "    for j=1:NMODELS\n",
    "        # FILL IN CODES HERE\n",
    "        throw(\"unimplemented\")\n",
    "    end;\n",
    "    variance=mean(variance)/NMODELS;\n",
    "    return(variance);\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#</GRADED>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>With the help of <strong>computevariance</strong> you can now compute the \"variance\" variable within <strong>biasvariancedemo</strong>. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>If you did everything correctly and call execute the following demo. You should see how the error decomposes (roughly) into bias, variance and noise when regularization constant $\\lambda$ increases.</p>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "361e02d2-5512-4517-9224-2fad387b5d60"
    }
   },
   "outputs": [],
   "source": [
    "# biasvariancedemo\n",
    "\n",
    "using PyPlot\n",
    "\n",
    "global Nsmall,Nbig,NMODELS,OFFSET;\n",
    "Nsmall=10;   # how big is the training set size N\n",
    "Nbig=10000;  # how big is a really big data set (approx. infinity)\n",
    "NMODELS=100; # how many models do you want to average over\n",
    "λs=-6:0.5:0; # What regularization constants to evaluate\n",
    "σ=4;         # what is the kernel width?\n",
    "\n",
    "# we store\n",
    "Nlambdas  = length(λs);\n",
    "lbias     = zeros(1,Nlambdas);\n",
    "lvariance = zeros(1,Nlambdas);\n",
    "ltotal    = zeros(1,Nlambdas);\n",
    "lnoise    = zeros(1,Nlambdas);\n",
    "lsum      = zeros(1,Nlambdas);\n",
    "\n",
    "\n",
    "## Different regularization constant classifiers\n",
    "for md=1:Nlambdas\n",
    "    λ=2^λs[md];\n",
    "    \n",
    "    # use this data set as an approximation of the true test set\n",
    "    xTe,yTe=toydata(OFFSET,Nbig);\n",
    "    \n",
    "    ## Estimate AVERAGE ERROR (TOTAL)\n",
    "    total=0;\n",
    "    for j=1:NMODELS\n",
    "        xTr2,yTr2=toydata(OFFSET,Nsmall);        \n",
    "        fsmall=kregression(xTr2,yTr2,σ,λ);\n",
    "        total=total+mean((fsmall(xTe)-yTe).^2);\n",
    "    end;\n",
    "    total=mean(total)/NMODELS;\n",
    "\n",
    "    \n",
    "    ## Estimate Noise\n",
    "    ybar=computeybar(xTe);\n",
    "    noise=mean((yTe-ybar).^2);\n",
    "    \n",
    "    ## Estimating BIAS\n",
    "    hbar=computehbar(xTe,σ, λ);\n",
    "    bias=mean((hbar-ybar).^2);\n",
    "    \n",
    "    ## Estimating VARIANCE\n",
    "    variance=computevariance(xTe,σ,λ,hbar);\n",
    "        \n",
    "    ## print and store results\n",
    "    lbias[md]=bias;\n",
    "    lvariance[md]=variance;\n",
    "    ltotal[md]=total;\n",
    "    lnoise[md]=noise;\n",
    "    lsum[md]=lbias[md]+lvariance[md]+lnoise[md];\n",
    "    @printf(\"Regularization λ=2^%2.1f: Bias: %2.4f Variance: %2.4f Noise: %2.4f Bias+Variance+Noise: %2.4f Test error: %2.4f\\n\",λs[md],lbias[md],lvariance[md],lnoise[md],lsum[md],ltotal[md]);\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "b1c8226f-08e7-4076-91a0-0a20c81cae6a"
    }
   },
   "outputs": [],
   "source": [
    "## plot results\n",
    "figure()\n",
    "plot(lbias[1:Nlambdas],\"r-\",linewidth=2);\n",
    "plot(lvariance[1:Nlambdas],\"k-\",linewidth=2);\n",
    "plot(lnoise[1:Nlambdas],\"g-\",linewidth=2);\n",
    "plot(ltotal[1:Nlambdas],\"b-\",linewidth=2);\n",
    "plot(lsum[1:Nlambdas],\"k--\",linewidth=2);\n",
    "\n",
    "legend([\"Bias\",\"Variance\",\"Noise\",\"Test error\",\"Bias+Var+Noise\"]);\n",
    "xlabel(\"Regularization \\lambda=2^x\",fontsize=18);\n",
    "ylabel(\"Squared Error\",fontsize=18);\n",
    "xticks(1:Nlambdas,λs);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "85a1738e-6706-473c-9be1-81e099d456ec"
    }
   },
   "source": [
    "Feel free to modify $\\lambda$/$\\sigma$ in these two files. If you want the approximation to be more accurate, increase <em>NMODELS</em> and/or <em>Nbig</em> (the more models you train, the better your approximation will be for $\\mathbb{E}[h]$ and $\\mathbb{E}[(h_D-\\bar{h})^2]$). \n",
    "You can also play around with the variable <em>Nsmall</em> which regulates how big your actual training is supposed to be. \n",
    "</p>\n",
    "\n",
    "\n",
    "<h3>Note</h3>\n",
    "<p>\n",
    "When computing the bias and variance, you approximate the results by training many $h_D$. We set <em>NMODELS</em>=1000 and use some thresholds to test if your functions' results are correct. Unfortunately, as a result of this randomness, there is still a small chance that you will fail some test cases, even though your implementations are correct. <br/><br/>\n",
    "If you can pass all the tests most of the times locally, then you are fine. In this case, if the autograder says your accuracy is not 100%, just commit the code again.<br/><br/>\n",
    "\n",
    "There is no competition this time.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  },
  "nbpresent": {
   "slides": {
    "a5bab2de-c519-4df9-93b1-1c93f8a6ec4f": {
     "id": "a5bab2de-c519-4df9-93b1-1c93f8a6ec4f",
     "prev": null,
     "regions": {}
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
